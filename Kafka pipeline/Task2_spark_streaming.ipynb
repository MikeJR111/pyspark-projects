{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:577)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1666)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:514)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:514)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:514)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b5e7d36a028b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#2. Then create a SparkSession using the SparkConf object. set UTC timezone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark_conf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.sql.session.timeZone\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"UTC\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mf:\\pycharm\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\pycharm\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\pycharm\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             self._do_init(\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mmaster\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mappName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\pycharm\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\pycharm\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \"\"\"\n\u001b[0;32m    416\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\pycharm\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1586\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1587\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1588\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[0;32m   1589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\pycharm\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:577)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1666)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:514)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:514)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:514)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "# Import SparkConf, sparkcontext, sparksession class into program\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#1. Create a SparkConf object for using 2 local cores \n",
    "master = \"local[2]\"\n",
    "app_name = \"ass2b_part2\"\n",
    "                                                                  \n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "#2. Then create a SparkSession using the SparkConf object. set UTC timezone\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\",\"UTC\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic from part1\n",
    "topic1 = \"ass2_part1_bureau\"\n",
    "topic2 = \"ass2_part1_customer\"\n",
    "#read both stream on local ip address\n",
    "bureau_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\", topic1) \\\n",
    "  .load()\n",
    "# assume them are all string \n",
    "bureau_df = bureau_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\", topic2) \\\n",
    "  .load()\n",
    "customer_df = customer_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 Q4, and Q5 \n",
    "\n",
    "The schema setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StringType, IntegerType,DoubleType,TimestampType,StructField,BooleanType\n",
    "# customer data schema\n",
    "customer_dataframe_schm = StructType([ \\\n",
    "    StructField(\"ID\",StringType(),True), \\\n",
    "    StructField(\"Frequency\",StringType(),True), \\\n",
    "    StructField(\"InstlmentMode\",StringType(),True), \\\n",
    "    StructField(\"LoanStatus\", StringType(), True), \\\n",
    "    StructField(\"PaymentMode\", StringType(), True), \\\n",
    "    StructField(\"BranchID\", StringType(), True), \\\n",
    "    StructField(\"Area\",StringType(),True), \\\n",
    "    StructField(\"Tenure\",IntegerType(),True), \\\n",
    "    StructField(\"AssetCost\",IntegerType(),True), \\\n",
    "    StructField(\"AmountFinance\",DoubleType(),True), \\\n",
    "    StructField(\"DisbursalAmount\",DoubleType(),True), \\\n",
    "    StructField(\"EMI\",DoubleType(),True), \\\n",
    "    StructField(\"DisbursalDate\",TimestampType(),True), \\\n",
    "    StructField(\"MaturityDAte\",TimestampType(),True), \\\n",
    "    StructField(\"AuthDate\",TimestampType(),True), \\\n",
    "    StructField(\"AssetID\",StringType(),True), \\\n",
    "    StructField(\"ManufacturerID\",StringType(),True), \\\n",
    "    StructField(\"SupplierID\",StringType(),True), \\\n",
    "    StructField(\"LTV\",DoubleType(),True), \\\n",
    "    StructField(\"SEX\",StringType(),True), \\\n",
    "    StructField(\"AGE\",IntegerType(),True), \\\n",
    "    StructField(\"MonthlyIncome\",DoubleType(),True), \\\n",
    "    StructField(\"City\",StringType(),True), \\\n",
    "    StructField(\"State\",StringType(),True), \\\n",
    "    StructField(\"ZiPCODE\",StringType(),True), \\\n",
    "    StructField(\"Top-up Month\",StringType(),True), \\\n",
    "    StructField('ts', TimestampType(), True) \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df=customer_df.select(F.from_json(F.col(\"value\").cast(\"string\"), customer_dataframe_schm).\\\n",
    "                               alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Frequency: string (nullable = true)\n",
      " |-- InstlmentMode: string (nullable = true)\n",
      " |-- LoanStatus: string (nullable = true)\n",
      " |-- PaymentMode: string (nullable = true)\n",
      " |-- BranchID: string (nullable = true)\n",
      " |-- Area: string (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- AssetCost: integer (nullable = true)\n",
      " |-- AmountFinance: double (nullable = true)\n",
      " |-- DisbursalAmount: double (nullable = true)\n",
      " |-- EMI: double (nullable = true)\n",
      " |-- DisbursalDate: timestamp (nullable = true)\n",
      " |-- MaturityDAte: timestamp (nullable = true)\n",
      " |-- AuthDate: timestamp (nullable = true)\n",
      " |-- AssetID: string (nullable = true)\n",
      " |-- ManufacturerID: string (nullable = true)\n",
      " |-- SupplierID: string (nullable = true)\n",
      " |-- LTV: double (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MonthlyIncome: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZiPCODE: string (nullable = true)\n",
      " |-- Top-up Month: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change the customer schema\n",
    "#customer_df = customer_df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))  \n",
    "customer_df_formatted = customer_df.select(\n",
    "                    F.col(\"parsed_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"parsed_value.Frequency\").alias(\"Frequency\"),\n",
    "                    F.col(\"parsed_value.InstlmentMode\").alias(\"InstlmentMode\"),\n",
    "                    F.col(\"parsed_value.LoanStatus\").alias(\"LoanStatus\"),\n",
    "                    F.col(\"parsed_value.PaymentMode\").alias(\"PaymentMode\"),\n",
    "                    F.col(\"parsed_value.BranchID\").alias(\"BranchID\"),\n",
    "                    F.col(\"parsed_value.Area\").alias(\"Area\"),\n",
    "                    F.col(\"parsed_value.Tenure\").alias(\"Tenure\"),\n",
    "                    F.col(\"parsed_value.AssetCost\").alias(\"AssetCost\"),\n",
    "                    F.col(\"parsed_value.AmountFinance\").alias(\"AmountFinance\"),\n",
    "                    F.col(\"parsed_value.DisbursalAmount\").alias(\"DisbursalAmount\"),\n",
    "                    F.col(\"parsed_value.EMI\").alias(\"EMI\"),\n",
    "                    F.col(\"parsed_value.DisbursalDate\").alias(\"DisbursalDate\"),\n",
    "                    F.col(\"parsed_value.MaturityDAte\").alias(\"MaturityDAte\"),\n",
    "                    F.col(\"parsed_value.AuthDate\").alias(\"AuthDate\"),\n",
    "                    F.col(\"parsed_value.AssetID\").alias(\"AssetID\"),\n",
    "                    F.col(\"parsed_value.ManufacturerID\").alias(\"ManufacturerID\"),\n",
    "                    F.col(\"parsed_value.SupplierID\").alias(\"SupplierID\"),\n",
    "                    F.col(\"parsed_value.LTV\").alias(\"LTV\"),\n",
    "                    F.col(\"parsed_value.SEX\").alias(\"SEX\"),\n",
    "                    F.col(\"parsed_value.AGE\").alias(\"AGE\"),\n",
    "                    F.col(\"parsed_value.MonthlyIncome\").alias(\"MonthlyIncome\"),\n",
    "                    F.col(\"parsed_value.City\").alias(\"City\"),\n",
    "                    F.col(\"parsed_value.State\").alias(\"State\"),\n",
    "                    F.col(\"parsed_value.ZiPCODE\").alias(\"ZiPCODE\"),\n",
    "                    F.col(\"parsed_value.Top-up Month\").alias(\"Top-up Month\"),\n",
    "                    F.col(\"parsed_value.ts\").alias(\"ts\")\n",
    "                )\n",
    "customer_df_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up bureau data schema \n",
    "bureau_dataframe_schm = StructType([ \\\n",
    "    StructField(\"ID\",StringType(),True), \\\n",
    "    StructField(\"SELF-INDICATOR\",BooleanType(),True), \\\n",
    "    StructField(\"MATCH-TYPE\",StringType(),True), \\\n",
    "    StructField(\"ACCT-TYPE\", StringType(), True), \\\n",
    "    StructField(\"CONTRIBUTOR-TYPE\", StringType(), True), \\\n",
    "    StructField(\"DATE-REPORTED\", TimestampType(), True), \\\n",
    "    StructField(\"OWNERSHIP-IND\",StringType(),True), \\\n",
    "    StructField(\"ACCOUNT-STATUS\",StringType(),True), \\\n",
    "    StructField(\"DISBURSED-DT\",TimestampType(),True), \\\n",
    "    StructField(\"CLOSE-DT\",TimestampType(),True), \\\n",
    "    StructField('LAST-PAYMENT-DATE',TimestampType(),True), \\\n",
    "    StructField('CREDIT-LIMIT/SANC AMT',IntegerType(),True), \\\n",
    "    StructField(\"DISBURSED-AMT/HIGH CREDIT\",IntegerType(),True), \\\n",
    "    StructField(\"INSTALLMENT-AMT\",StringType(),True), \\\n",
    "    StructField(\"CURRENT-BAL\",IntegerType(),True), \\\n",
    "    StructField(\"INSTALLMENT-FREQUENCY\",StringType(),True), \\\n",
    "    StructField(\"OVERDUE-AMT\",IntegerType(),True), \\\n",
    "    StructField(\"WRITE-OFF-AMT\",DoubleType(),True), \\\n",
    "    StructField(\"ASSET_CLASS\",StringType(),True), \\\n",
    "    StructField(\"REPORTED DATE - HIST\",StringType(),True), \\\n",
    "    StructField(\"DPD - HIST\",StringType(),True), \\\n",
    "    StructField(\"CUR BAL - HIST\",StringType(),True), \\\n",
    "    StructField(\"AMT OVERDUE - HIST\",StringType(),True), \\\n",
    "    StructField(\"AMT PAID - HIST\",StringType(),True), \\\n",
    "    StructField(\"TENURE\",IntegerType(),True), \\\n",
    "    StructField('ts', TimestampType(), True) \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_df=bureau_df.select(F.from_json(F.col(\"value\").cast(\"string\"), bureau_dataframe_schm).alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- SELF-INDICATOR: boolean (nullable = true)\n",
      " |-- MATCH-TYPE: string (nullable = true)\n",
      " |-- ACCT-TYPE: string (nullable = true)\n",
      " |-- CONTRIBUTOR-TYPE: string (nullable = true)\n",
      " |-- DATE-REPORTED: timestamp (nullable = true)\n",
      " |-- OWNERSHIP-IND: string (nullable = true)\n",
      " |-- ACCOUNT-STATUS: string (nullable = true)\n",
      " |-- DISBURSED-DT: timestamp (nullable = true)\n",
      " |-- CLOSE-DT: timestamp (nullable = true)\n",
      " |-- LAST-PAYMENT-DATE: timestamp (nullable = true)\n",
      " |-- CREDIT-LIMIT/SANC AMT: integer (nullable = true)\n",
      " |-- DISBURSED-AMT/HIGH CREDIT: integer (nullable = true)\n",
      " |-- INSTALLMENT-AMT: string (nullable = true)\n",
      " |-- CURRENT-BAL: integer (nullable = true)\n",
      " |-- INSTALLMENT-FREQUENCY: string (nullable = true)\n",
      " |-- OVERDUE-AMT: integer (nullable = true)\n",
      " |-- WRITE-OFF-AMT: double (nullable = true)\n",
      " |-- ASSET_CLASS: string (nullable = true)\n",
      " |-- REPORTED DATE - HIST: string (nullable = true)\n",
      " |-- DPD - HIST: string (nullable = true)\n",
      " |-- CUR BAL - HIST: string (nullable = true)\n",
      " |-- AMT OVERDUE - HIST: string (nullable = true)\n",
      " |-- AMT PAID - HIST: string (nullable = true)\n",
      " |-- TENURE: integer (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change bureau data schema \n",
    "bureau_df_formatted = bureau_df.select(\n",
    "                    F.col(\"parsed_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"parsed_value.SELF-INDICATOR\").alias(\"SELF-INDICATOR\"),\n",
    "                    F.col(\"parsed_value.MATCH-TYPE\").alias(\"MATCH-TYPE\"),\n",
    "                    F.col(\"parsed_value.ACCT-TYPE\").alias(\"ACCT-TYPE\"),\n",
    "                    F.col(\"parsed_value.CONTRIBUTOR-TYPE\").alias(\"CONTRIBUTOR-TYPE\"),\n",
    "                    F.col(\"parsed_value.DATE-REPORTED\").alias(\"DATE-REPORTED\"),\n",
    "                    F.col(\"parsed_value.OWNERSHIP-IND\").alias(\"OWNERSHIP-IND\"),\n",
    "                    F.col(\"parsed_value.ACCOUNT-STATUS\").alias(\"ACCOUNT-STATUS\"),\n",
    "                    F.col(\"parsed_value.DISBURSED-DT\").alias(\"DISBURSED-DT\"),\n",
    "                    F.col(\"parsed_value.CLOSE-DT\").alias(\"CLOSE-DT\"),\n",
    "                    F.col(\"parsed_value.LAST-PAYMENT-DATE\").alias(\"LAST-PAYMENT-DATE\"),\n",
    "                    F.col(\"parsed_value.CREDIT-LIMIT/SANC AMT\").alias(\"CREDIT-LIMIT/SANC AMT\"),\n",
    "                    F.col(\"parsed_value.DISBURSED-AMT/HIGH CREDIT\").alias(\"DISBURSED-AMT/HIGH CREDIT\"),\n",
    "                    F.col(\"parsed_value.INSTALLMENT-AMT\").alias(\"INSTALLMENT-AMT\"),\n",
    "                    F.col(\"parsed_value.CURRENT-BAL\").alias(\"CURRENT-BAL\"),\n",
    "                    F.col(\"parsed_value.INSTALLMENT-FREQUENCY\").alias(\"INSTALLMENT-FREQUENCY\"),\n",
    "                    F.col(\"parsed_value.OVERDUE-AMT\").alias(\"OVERDUE-AMT\"),\n",
    "                    F.col(\"parsed_value.WRITE-OFF-AMT\").alias(\"WRITE-OFF-AMT\"),\n",
    "                    F.col(\"parsed_value.ASSET_CLASS\").alias(\"ASSET_CLASS\"),\n",
    "                    F.col(\"parsed_value.REPORTED DATE - HIST\").alias(\"REPORTED DATE - HIST\"),\n",
    "                    F.col(\"parsed_value.DPD - HIST\").alias(\"DPD - HIST\"),\n",
    "                    F.col(\"parsed_value.CUR BAL - HIST\").alias(\"CUR BAL - HIST\"),\n",
    "                    F.col(\"parsed_value.AMT OVERDUE - HIST\").alias(\"AMT OVERDUE - HIST\"),\n",
    "                    F.col(\"parsed_value.AMT PAID - HIST\").alias(\"AMT PAID - HIST\"),\n",
    "                    F.col(\"parsed_value.TENURE\").alias(\"TENURE\"),\n",
    "                    F.col(\"parsed_value.ts\").alias(\"ts\")\n",
    "                )\n",
    "bureau_df_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some data\n",
    "bureau_query = bureau_df_formatted \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "customer_query = customer_df_formatted \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_query.stop()\n",
    "bureau_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "watermark, update column and join dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "## updata  SELF-INDICATOR\n",
    "bureau_df_formatted = bureau_df_formatted.withColumn(\"SELF-INDICATOR\",\n",
    "                                                     when(bureau_df_formatted['SELF-INDICATOR'] == \"TRUE\",1)\\\n",
    "                               .when(bureau_df_formatted['SELF-INDICATOR'] == \"FALSE\",0))\n",
    "# bureau_df_formatted = bureau_df_formatted.withColumn(\"SELF-INDICATOR\",bureau_df_formatted['SELF-INDICATOR']\\\n",
    "#                                                      .cast(\"boolean\"))\n",
    "\n",
    "#for bureau use ts as watermark, set 5s delay\n",
    "#group data based on ID and 30s window duration, sum numeric column and count distinct value for other column\n",
    "# and rename column\n",
    "\n",
    "bureau_water = bureau_df_formatted \\\n",
    "    .withWatermark(\"ts\", \"5 seconds\")\\\n",
    "    .groupBy(\"ID\", F.window(\"ts\", \"30 seconds\"))\\\n",
    "    .agg(F.sum(\"CREDIT-LIMIT/SANC AMT\").alias(\"CREDIT-LIMIT/SANC AMT_sum\"),\n",
    "                           F.sum(\"DISBURSED-AMT/HIGH CREDIT\").alias(\"DISBURSED-AMT/HIGH CREDIT_sum\"),\n",
    "                           F.sum(\"CURRENT-BAL\").alias(\"CURRENT-BAL_sum\"),\n",
    "                           F.sum(\"OVERDUE-AMT\").alias(\"OVERDUE-AMT_sum\"),\n",
    "                           F.sum(\"TENURE\").alias(\"TENURE_sum\"),\n",
    "                           F.sum(\"WRITE-OFF-AMT\").alias(\"WRITE-OFF-AMT_sum\"),\n",
    "                           F.sum(\"SELF-INDICATOR\").alias(\"SELF-INDICATOR_sum\"),\n",
    "                           F.approx_count_distinct(\"MATCH-TYPE\").alias(\"MATCH-TYPE_dist\"),\n",
    "                           F.approx_count_distinct(\"ACCT-TYPE\").alias(\"ACCT-TYPE_dist\"),\n",
    "                           F.approx_count_distinct(\"CONTRIBUTOR-TYPE\").alias(\"CONTRIBUTOR-TYPE_dist\"),\n",
    "                           F.approx_count_distinct(\"DATE-REPORTED\").alias(\"DATE-REPORTED_dist\"),\n",
    "                           F.approx_count_distinct(\"OWNERSHIP-IND\").alias(\"OWNERSHIP-IND_dist\"),\n",
    "                           F.approx_count_distinct(\"ACCOUNT-STATUS\").alias(\"ACCOUNT-STATUS_dist\"),\n",
    "                           F.approx_count_distinct(\"DISBURSED-DT\").alias(\"DISBURSED-DT_dist\"),\n",
    "                           F.approx_count_distinct(\"CLOSE-DT\").alias(\"CLOSE-DT_dist\"),\n",
    "                           F.approx_count_distinct(\"LAST-PAYMENT-DATE\").alias(\"LAST-PAYMENT-DATE_dist\"),\n",
    "                           F.approx_count_distinct(\"INSTALLMENT-AMT\").alias(\"INSTALLMENT-AMT_dist\"),\n",
    "                           F.approx_count_distinct(\"INSTALLMENT-FREQUENCY\").alias(\"INSTALLMENT-FREQUENCY_dist\"),\n",
    "                           F.approx_count_distinct(\"ASSET_CLASS\").alias(\"ASSET_CLASS_dist\"),\n",
    "                           F.approx_count_distinct(\"REPORTED DATE - HIST\").alias(\"REPORTED DATE - HIST_dist\"),\n",
    "                           F.approx_count_distinct(\"DPD - HIST\").alias(\"DPD - HIST_dist\"),\n",
    "                           F.approx_count_distinct(\"CUR BAL - HIST\").alias(\"CUR BAL - HIST_dist\"),\n",
    "                           F.approx_count_distinct(\"AMT OVERDUE - HIST\").alias(\"AMT OVERDUE - HIST_dist\"),\n",
    "                           F.approx_count_distinct(\"AMT PAID - HIST\").alias(\"AMT PAID - HIST_dist\"))\n",
    "customer_water = customer_df_formatted \\\n",
    "    .withWatermark(\"ts\", \"5 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- CREDIT-LIMIT/SANC AMT_sum: long (nullable = true)\n",
      " |-- DISBURSED-AMT/HIGH CREDIT_sum: long (nullable = true)\n",
      " |-- CURRENT-BAL_sum: long (nullable = true)\n",
      " |-- OVERDUE-AMT_sum: long (nullable = true)\n",
      " |-- TENURE_sum: long (nullable = true)\n",
      " |-- WRITE-OFF-AMT_sum: double (nullable = true)\n",
      " |-- SELF-INDICATOR_sum: long (nullable = true)\n",
      " |-- MATCH-TYPE_dist: long (nullable = false)\n",
      " |-- ACCT-TYPE_dist: long (nullable = false)\n",
      " |-- CONTRIBUTOR-TYPE_dist: long (nullable = false)\n",
      " |-- DATE-REPORTED_dist: long (nullable = false)\n",
      " |-- OWNERSHIP-IND_dist: long (nullable = false)\n",
      " |-- ACCOUNT-STATUS_dist: long (nullable = false)\n",
      " |-- DISBURSED-DT_dist: long (nullable = false)\n",
      " |-- CLOSE-DT_dist: long (nullable = false)\n",
      " |-- LAST-PAYMENT-DATE_dist: long (nullable = false)\n",
      " |-- INSTALLMENT-AMT_dist: long (nullable = false)\n",
      " |-- INSTALLMENT-FREQUENCY_dist: long (nullable = false)\n",
      " |-- ASSET_CLASS_dist: long (nullable = false)\n",
      " |-- REPORTED DATE - HIST_dist: long (nullable = false)\n",
      " |-- DPD - HIST_dist: long (nullable = false)\n",
      " |-- CUR BAL - HIST_dist: long (nullable = false)\n",
      " |-- AMT OVERDUE - HIST_dist: long (nullable = false)\n",
      " |-- AMT PAID - HIST_dist: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bureau_water.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns \"window_start and window_end\" then join df based on ID\n",
    "bureau_grouped_sum_dist = bureau_water.select(\n",
    "                    F.col(\"window.start\").alias(\"window_start\"),\n",
    "                    F.col(\"window.end\").alias(\"window_end\"),\n",
    "                    F.col(\"*\")\n",
    "                ).drop(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sql tempview\n",
    "bureau_grouped_sum_dist.createOrReplaceTempView(\"bureau_grouped_sum_dist\")\n",
    "customer_water.createOrReplaceTempView(\"customer_water\")\n",
    "#then join df based on ID and filter customer data not received within window \n",
    "join_df = spark.sql('''\n",
    "    SELECT *\n",
    "    \n",
    "    FROM bureau_grouped_sum_dist b INNER JOIN customer_water c  ON b.ID = c.ID\n",
    "    WHERE c.ts >= b.window_start AND c.ts <=window_end;\n",
    "                    ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- CREDIT-LIMIT/SANC AMT_sum: long (nullable = true)\n",
      " |-- DISBURSED-AMT/HIGH CREDIT_sum: long (nullable = true)\n",
      " |-- CURRENT-BAL_sum: long (nullable = true)\n",
      " |-- OVERDUE-AMT_sum: long (nullable = true)\n",
      " |-- TENURE_sum: long (nullable = true)\n",
      " |-- WRITE-OFF-AMT_sum: double (nullable = true)\n",
      " |-- SELF-INDICATOR_sum: long (nullable = true)\n",
      " |-- MATCH-TYPE_dist: long (nullable = false)\n",
      " |-- ACCT-TYPE_dist: long (nullable = false)\n",
      " |-- CONTRIBUTOR-TYPE_dist: long (nullable = false)\n",
      " |-- DATE-REPORTED_dist: long (nullable = false)\n",
      " |-- OWNERSHIP-IND_dist: long (nullable = false)\n",
      " |-- ACCOUNT-STATUS_dist: long (nullable = false)\n",
      " |-- DISBURSED-DT_dist: long (nullable = false)\n",
      " |-- CLOSE-DT_dist: long (nullable = false)\n",
      " |-- LAST-PAYMENT-DATE_dist: long (nullable = false)\n",
      " |-- INSTALLMENT-AMT_dist: long (nullable = false)\n",
      " |-- INSTALLMENT-FREQUENCY_dist: long (nullable = false)\n",
      " |-- ASSET_CLASS_dist: long (nullable = false)\n",
      " |-- REPORTED DATE - HIST_dist: long (nullable = false)\n",
      " |-- DPD - HIST_dist: long (nullable = false)\n",
      " |-- CUR BAL - HIST_dist: long (nullable = false)\n",
      " |-- AMT OVERDUE - HIST_dist: long (nullable = false)\n",
      " |-- AMT PAID - HIST_dist: long (nullable = false)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Frequency: string (nullable = true)\n",
      " |-- InstlmentMode: string (nullable = true)\n",
      " |-- LoanStatus: string (nullable = true)\n",
      " |-- PaymentMode: string (nullable = true)\n",
      " |-- BranchID: string (nullable = true)\n",
      " |-- Area: string (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- AssetCost: integer (nullable = true)\n",
      " |-- AmountFinance: double (nullable = true)\n",
      " |-- DisbursalAmount: double (nullable = true)\n",
      " |-- EMI: double (nullable = true)\n",
      " |-- DisbursalDate: timestamp (nullable = true)\n",
      " |-- MaturityDAte: timestamp (nullable = true)\n",
      " |-- AuthDate: timestamp (nullable = true)\n",
      " |-- AssetID: string (nullable = true)\n",
      " |-- ManufacturerID: string (nullable = true)\n",
      " |-- SupplierID: string (nullable = true)\n",
      " |-- LTV: double (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- MonthlyIncome: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZiPCODE: string (nullable = true)\n",
      " |-- Top-up Month: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column and rename column\n",
    "join_df_1 = join_df \\\n",
    "    .select(\"b.ID\",\"window_start\",\"Top-up Month\",\"window_end\",\"ts\")\n",
    "join_df_1 = join_df_1.withColumnRenamed(\"Top-up Month\", \"Top-up_Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into parquet files\n",
    "join_df_sink = join_df_1.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"True_data\")\\\n",
    "        .option(\"checkpointLocation\", \"True_data/checkpoint\")\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the query\n",
    "join_df_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n",
      "+------+-------------------+-----------------+-------------------+-------------------+\n",
      "|    ID|       window_start|     Top-up_Month|         window_end|                 ts|\n",
      "+------+-------------------+-----------------+-------------------+-------------------+\n",
      "|105027|2022-10-19 05:50:30|No Top-up Service|2022-10-19 05:51:00|2022-10-19 05:50:48|\n",
      "|113865|2022-10-19 05:50:30|No Top-up Service|2022-10-19 05:51:00|2022-10-19 05:50:48|\n",
      "|129791|2022-10-19 05:51:00|No Top-up Service|2022-10-19 05:51:30|2022-10-19 05:51:12|\n",
      "|109424|2022-10-19 05:51:00|No Top-up Service|2022-10-19 05:51:30|2022-10-19 05:51:12|\n",
      "|127812|2022-10-19 05:51:30|No Top-up Service|2022-10-19 05:52:00|2022-10-19 05:51:30|\n",
      "|124394|2022-10-19 05:51:30|No Top-up Service|2022-10-19 05:52:00|2022-10-19 05:51:30|\n",
      "|116990|2022-10-19 05:29:00|No Top-up Service|2022-10-19 05:29:30|2022-10-19 05:29:25|\n",
      "|101123|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:31|\n",
      "|112714|2022-10-19 05:29:00|No Top-up Service|2022-10-19 05:29:30|2022-10-19 05:29:07|\n",
      "|107185|2022-10-19 05:29:30|No Top-up Service|2022-10-19 05:30:00|2022-10-19 05:29:30|\n",
      "|105745|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:55|\n",
      "|123899|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:43|\n",
      "|121760|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:55|\n",
      "|112226|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:31|\n",
      "|137866|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:49|\n",
      "|108928|2022-10-19 05:29:00|No Top-up Service|2022-10-19 05:29:30|2022-10-19 05:29:24|\n",
      "|116692|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:49|\n",
      "|115969|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:38|\n",
      "|105066|2022-10-19 05:28:30|No Top-up Service|2022-10-19 05:29:00|2022-10-19 05:28:55|\n",
      "|102005|2022-10-19 05:29:30|No Top-up Service|2022-10-19 05:30:00|2022-10-19 05:29:30|\n",
      "+------+-------------------+-----------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the dataframe\n",
    "true_data = spark.read.parquet(\"True_data\")\n",
    "true_data.printSchema()\n",
    "true_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load machine learning model\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "model = PipelineModel.load('topup_pipeline_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tGBTClassifier_faaeb7a4e7a4-featuresCol: features,\n",
      "\tGBTClassifier_faaeb7a4e7a4-labelCol: Top-up\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model.stages[-1]._java_obj.paramMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the pipeline model to make prediction on data\n",
    "# select column and rename\n",
    "predict = model.transform(join_df).select(\"b.ID\",\"window_start\",\"Top-up Month\",\"window_end\",\"ts\",\"prediction\").\\\n",
    "            withColumnRenamed(\"Top-up Month\", \"Top-up_Month\")\n",
    "predict.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data in a parquet file \n",
    "predict_sink = predict.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"predict\")\\\n",
    "        .option(\"checkpointLocation\", \"predict/checkpoint\")\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- Top-up_Month: string (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+---+------------+------------+----------+---+----------+\n",
      "| ID|window_start|Top-up_Month|window_end| ts|prediction|\n",
      "+---+------------+------------+----------+---+----------+\n",
      "+---+------------+------------+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_data = spark.read.parquet(\"predict\")\n",
    "predict_data.printSchema()\n",
    "predict_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bureau_query = bureau_df_formatted \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .trigger(processingTime='5 seconds') \\\n",
    "#     .start()\n",
    "# customer_query = customer_df_formatted \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .trigger(processingTime='5 seconds') \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_query.stop()\n",
    "# bureau_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
